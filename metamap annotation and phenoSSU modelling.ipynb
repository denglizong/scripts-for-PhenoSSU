{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pymetamap import MetaMap\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus path\n",
    "corpus_path = '/home/denglizong/venv4attr/corpus/WikiPediaR5'\n",
    "# os.listdir( corpus_path )[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Acinetobacter infections',\n 'Actinomycosis',\n 'African sleeping sickness (African trypanosomiasis)']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# list of disease names\n",
    "list_of_diseases = [ filename.replace('.txt',\"\") \n",
    "                    for filename in os.listdir( corpus_path ) if filename.endswith(\".txt\")]\n",
    "#\n",
    "list_of_diseases[0:3]                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate corpus with metamap \n",
    "# explore optimal parameters of metamap based on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat instance of metamap\n",
    "mm = MetaMap.get_instance(\"/home/denglizong/public_mm/bin/metamap20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict scope\n",
    "semantic_group_of_disorder = ['acab','anab','cgab','comd','dsyn','emod','fndg','inpo','mobd','patf','sosy']\n",
    "semantic_group_of_anatomy  = ['anst','bdsu','bdsy','bpoc','bsoj','emst','ffas']\n",
    "restrict_to_sts = semantic_group_of_disorder + semantic_group_of_anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict sources\n",
    "# restrict_to_sources = ['SNOMEDCT_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# help( mm.extract_concepts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 Acinetobacter infections 2 10\n1 Actinomycosis 4 5\n2 African sleeping sickness (African trypanosomiasis) 16 22\n3 AIDS (acquired immunodeficiency syndrome) 37 34\n4 Amoebiasis 21 8\n5 Anaplasmosis 6 10\n6 Angiostrongyliasis 9 24\n7 Anthrax 32 50\n8 Argentine hemorrhagic fever 8 4\n9 Ascariasis 15 21\n10 Aspergillosis 12 23\n11 Astrovirus infection 10 11\n12 Babesiosis 13 46\n13 Bacterial meningitis 39 73\n14 Bacterial pneumonia 1 9\n15 Bacterial vaginosis 18 12\n16 Balantidiasis 1 10\n17 Bartonellosis 31 56\n18 Baylisascaris infection 1 11\n19 Bejel 2 1\n20 BK virus infection 10 9\n21 Blastocystosis 9 9\n22 Blastomycosis 2 27\n23 Bolivian hemorrhagic fever 6 11\n24 Botulism (and Infant botulism) 28 33\n25 Brucellosis 8 33\n26 Bubonic plague 8 17\n27 Buruli ulcer 7 9\n28 Campylobacteriosis 14 25\n29 Candidiasis (Moniliasis; Thrush) 17 29\n30 Carrion's disease 26 31\n31 Cat-scratch disease 18 28\n32 Cellulitis 3 7\n33 Chagas disease (American trypanosomiasis) 30 39\n34 Chancroid 9 10\n35 Chickenpox 18 36\n36 Chikungunya 42 53\n37 Chlamydia 24 38\n38 Chromoblastomycosis 9 3\n39 Clostridium difficile colitis 5 19\n40 Coccidioidomycosis 17 41\n41 Colorado tick fever (CTF) 7 19\n42 Common cold (Acute viral rhinopharyngitis; Acute coryza) 13 24\n43 Coronavirus disease 2019 (COVID-19) 29 42\n44 Creutzfeldt–Jakob disease 11 16\n45 Crimean-Congo hemorrhagic fever (CCHF) 13 25\n46 Cryptococcosis 11 17\n47 Cryptosporidiosis 8 30\n48 Cutaneous larva migrans (CLM) 8 7\n49 Cyclosporiasis 6 5\n50 Cysticercosis 16 17\n51 Dengue fever 32 51\n52 Dientamoebiasis 5 15\n53 Diphtheria 11 23\n54 Dracunculiasis 13 19\n55 Ebola hemorrhagic fever 25 40\n56 Echinococcosis 14 20\n57 Ehrlichiosis 10 12\n58 Enterobiasis (Pinworm infection) 18 20\n59 Epidemic typhus 5 16\n60 Epstein–Barr virus infectious mononucleosis (Mono) 38 46\n61 Erythema infectiosum (Fifth disease) 14 20\n62 Exanthem subitum (Sixth disease) 11 15\n63 Fasciolasis 16 34\n64 Fasciolopsiasis 3 6\n65 Fatal familial insomnia (FFI) 10 15\n66 Filariasis 7 13\n67 Free-living amebic infection 4 8\n68 Gas gangrene (Clostridial myonecrosis) 3 4\n69 Geotrichosis 29 18\n70 Gerstmann-Sträussler-Scheinker syndrome (GSS) 8 16\n71 Giardiasis 15 21\n72 Glanders 3 6\n73 Gnathostomiasis 8 18\n74 Gonorrhea 18 21\n75 Granuloma inguinale (Donovanosis) 9 1\n76 Haemophilus influenzae infection 8 18\n77 Hand, foot and mouth disease (HFMD) 7 15\n78 Hantavirus Pulmonary Syndrome (HPS) 3 9\n79 Heartland virus disease 3 13\n80 Helicobacter pylori infection 17 35\n81 Hemolytic-uremic syndrome (HUS) 10 41\n82 Hemorrhagic fever with renal syndrome (HFRS) 13 33\n83 Hepatitis A 8 18\n84 Hepatitis B 18 35\n85 Hepatitis C 44 70\n86 Hepatitis E 19 26\n87 Herpes simplex 32 48\n88 Histoplasmosis 15 24\n89 Human ewingii ehrlichiosis 2 6\n90 Human granulocytic anaplasmosis (HGA) 6 24\n91 Human monocytic ehrlichiosis 6 12\n92 Human papillomavirus (HPV) infection 31 15\n93 Hymenolepiasis 12 23\n94 Influenza (flu) 18 50\n95 Intestinal capillariasis 4 4\n96 Isosporiasis 4 7\n97 Kawasaki disease 82 150\n98 Kuru 18 19\n99 Lassa fever 12 13\n100 Legionellosis (Legionnaires' disease) 13 26\n101 Leishmaniasis 6 8\n102 Leprosy 14 16\n103 Leptospirosis 34 59\n104 Listeriosis 8 16\n105 Lyme disease (Lyme borreliosis) 65 108\n106 Lymphatic filariasis (Elephantiasis) 12 17\n107 Lymphocytic choriomeningitis 16 41\n108 Malaria 9 22\n109 Marburg hemorrhagic fever (MHF) 5 17\n110 Measles 25 33\n111 Melioidosis (Whitmore's disease) 45 66\n112 Meningitis 39 73\n113 Meningococcal disease 10 36\n114 Metagonimiasis 13 20\n115 Middle East respiratory syndrome (MERS) 10 19\n116 Molluscum contagiosum (MC) 9 7\n117 Mumps 17 26\n118 Murine typhus (Endemic typhus) 4 13\n119 Mycoplasma genitalium infection 12 10\n120 Myiasis 16 17\n121 Neonatal conjunctivitis (Ophthalmia neonatorum) 3 7\n122 Nipah virus infection 7 9\n123 Nocardiosis 1 21\n124 Norovirus (children and babies) 9 15\n125 Onchocerciasis (River blindness) 24 39\n126 Opisthorchiasis 18 18\n127 Paracoccidioidomycosis (South American blastomycosis) 14 14\n128 Paragonimiasis 9 10\n129 Pediculosis capitis (Head lice) 9 8\n130 Pediculosis pubis (pubic lice, crab lice) 5 4\n131 Pelvic inflammatory disease (PID) 6 14\n132 Pertussis (whooping cough) 15 27\n133 Plague 21 19\n134 Pneumocystis pneumonia (PCP) 5 17\n135 Pneumonia 17 38\n136 Poliomyelitis 10 29\n137 Primary amoebic meningoencephalitis (PAM) 8 13\n138 Progressive multifocal leukoencephalopathy 4 2\n139 Psittacosis 12 37\n140 Q fever 10 31\n141 Rabies 17 24\n142 Relapsing fever 6 6\n143 Respiratory syncytial virus infection 8 20\n144 Rhinosporidiosis 1 8\n145 Rickettsialpox 2 4\n146 Rift Valley fever (RVF) 9 21\n147 Rocky Mountain spotted fever (RMSF) 13 34\n148 Rotavirus infection 11 11\n149 Rubella 24 43\n150 Salmonellosis 26 40\n151 Sapovirus 7 10\n152 SARS (severe acute respiratory syndrome) 5 8\n153 Scabies 22 14\n154 Scarlet fever 57 95\n155 Schistosomiasis 41 62\n156 Sepsis 5 21\n157 Shigellosis (bacillary dysentery) 5 9\n158 Shingles 33 54\n159 Smallpox 72 80\n160 Sporotrichosis 7 10\n161 Staphylococcal infection 8 21\n162 Strongyloidiasis 25 23\n163 Subacute sclerosing panencephalitis 6 15\n164 Syphilis 46 44\n165 Taeniasis 20 28\n166 Tetanus (lockjaw) 38 55\n167 Tinea barbae (barber's itch) 4 7\n168 Tinea capitis (ringworm of the scalp) 15 10\n169 Tinea corporis 3 3\n170 Tinea cruris (Jock itch) 9 9\n171 Tinea pedis (athlete’s foot) 27 33\n172 Tinea unguium (onychomycosis) 9 14\n173 Tinea versicolor (Pityriasis versicolor) 6 8\n174 Toxocariasis (ocular larva migrans (OLM)) 27 49\n175 Toxoplasmosis 26 35\n176 Trachoma 14 20\n177 Trichinosis 12 21\n178 Trichomoniasis 12 11\n179 Trichuriasis 12 18\n180 Tuberculosis 20 30\n181 Tularemia 9 7\n182 Typhoid fever 23 45\n183 Typhus fever 7 7\n184 Valley fever 17 41\n185 Variant Creutzfeldt–Jakob disease 4 2\n186 Vibrio parahaemolyticus enteritis 5 6\n187 Vibrio vulnificus infection 13 26\n188 Viral pneumonia 3 6\n189 Yaws 5 1\n190 Yellow fever 9 21\n191 Yersiniosis 5 13\n192 Zika fever 20 18\nCPU times: user 1.24 s, sys: 1.86 s, total: 3.1 s\nWall time: 13min 4s\n"
    }
   ],
   "source": [
    "%%time\n",
    "# given .txt and .ann\n",
    "# output annotation results in text file\n",
    "dict_of_concepts_in_texts = {}\n",
    "\n",
    "for index, disease_name in enumerate(list_of_diseases):\n",
    "    # text file \n",
    "    textfile = os.path.join( corpus_path, disease_name + \".txt\" )\n",
    "\n",
    "    # text string\n",
    "    textstr = \"\"\n",
    "    with open(textfile, 'r', encoding='utf-8') as f:\n",
    "        textstr = f.read()\n",
    "    # repalce '\\n' to ','\n",
    "    textstr = textstr.replace('\\n',',')\n",
    "\n",
    "    # sentence tokonize with position\n",
    "    text_sents = []\n",
    "    text_sents_with_spans = []\n",
    "    for start, end in PunktSentenceTokenizer().span_tokenize(textstr):\n",
    "        text_sents_with_spans.append( (start,end, textstr[start:end]) )\n",
    "        text_sents.append( textstr[start:end] )\n",
    "    # print( len(text_sents) )\n",
    "    # print( text_sents[0] )\n",
    "\n",
    "\n",
    "    # sentence index\n",
    "    sents_idx = range( len(text_sents) )\n",
    "    # print( list(sents_idx) )\n",
    "\n",
    "    # annotation sentences with metamap\n",
    "    # 标注参数 composite_phrase = 2, allow_concept_gaps=True, ignore_word_order=True (aim to improve recall)\n",
    "    # concepts,error = mm.extract_concepts(text_sents, list(sents_idx), restrict_to_sts=restrict_to_sts, \\\n",
    "    #                                     composite_phrase=2, allow_concept_gaps=True, ignore_word_order=True, no_nums = ['fndg'], \\\n",
    "    #                                     word_sense_disambiguation=True)\n",
    "    # \n",
    "    # change parameters\n",
    "    # composite_phrase=0, allow_concept_gaps=False, ignore_word_order=False\n",
    "    # SNOMEDCT_US, HPO\n",
    "    concepts,error = mm.extract_concepts(text_sents, list(sents_idx), \\\n",
    "                                        restrict_to_sts=restrict_to_sts, restrict_to_sources=['HPO'], \\\n",
    "                                        composite_phrase=2, allow_concept_gaps=False, ignore_word_order=False, no_nums = ['fndg'], \\\n",
    "                                        word_sense_disambiguation=True)\n",
    "\n",
    "    # change parameters\n",
    "    # concepts,error = mm.extract_concepts(text_sents, list(sents_idx), restrict_to_sts=restrict_to_sts, \\\n",
    "    #                                     composite_phrase=0, allow_concept_gaps=False, ignore_word_order=True, no_nums = ['fndg'], \\\n",
    "    #                                     word_sense_disambiguation=True)    \n",
    "\n",
    "    \n",
    "    # save annotation result\n",
    "    print(index, disease_name, len(text_sents), len(concepts) )\n",
    "    dict_of_concepts_in_texts.setdefault( disease_name, concepts )                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish in 17min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save annotation results to json file\n",
    "save_path = '/home/denglizong/SSUMiner/corpus/MetaMapAnnotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for disease_name in dict_of_concepts_in_texts:\n",
    "    # list of concepts (as dict) in the text\n",
    "    list_of_concepts_in_text = []\n",
    "\n",
    "    #\n",
    "    concepts = dict_of_concepts_in_texts[disease_name]\n",
    "    for concept in concepts:\n",
    "        # keep concept with mm tag\n",
    "        if 'mm' in dir(concept):\n",
    "            # 记录该concept的信息为词典形式\n",
    "            tmpdict = {}    \n",
    "            #\n",
    "            tmpdict.setdefault( \"index\", concept.index ) \n",
    "            tmpdict.setdefault( \"score\", concept.score ) \n",
    "            tmpdict.setdefault( \"preferred_name\", concept.preferred_name ) \n",
    "            tmpdict.setdefault( \"cui\", concept.cui ) \n",
    "            tmpdict.setdefault( \"semtypes\", concept.semtypes ) \n",
    "            tmpdict.setdefault( \"trigger\", concept.trigger ) \n",
    "            tmpdict.setdefault( \"pos_info\", concept.pos_info )     \n",
    "            tmpdict.setdefault( \"tree_codes\", concept.tree_codes )     \n",
    "            #\n",
    "            # \n",
    "            if len(tmpdict) != 0:\n",
    "                list_of_concepts_in_text.append( tmpdict )    \n",
    "\n",
    "    # \n",
    "    outfilepath = os.path.join( save_path, disease_name+'.json' )\n",
    "    json.dump(list_of_concepts_in_text, open(outfilepath,'w',encoding='utf-8'),indent=2,ensure_ascii=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration for metamap annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare metamap with metamap lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute performance of matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names for training and testing \n",
    "data_dir = \"/home/denglizong/SSUMiner/corpus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(133, 60)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# disease names in training and testing set\n",
    "diseases_used_for_training = []\n",
    "diseases_used_for_test = []\n",
    "\n",
    "# training \n",
    "file_of_diseases_for_training = os.path.join( data_dir+\"TrainTestSplit\", \"diseases_for_training.txt\" )\n",
    "with open( file_of_diseases_for_training, 'r', encoding='utf-8' ) as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip() != \"\":\n",
    "            diseases_used_for_training.append( line.strip()  )\n",
    "\n",
    "# testing\n",
    "file_of_diseases_for_test = os.path.join( data_dir+\"TrainTestSplit\", \"diseases_for_test.txt\" )\n",
    "with open( file_of_diseases_for_test, 'r', encoding='utf-8' ) as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip() != \"\":\n",
    "            diseases_used_for_test.append( line.strip()  )\n",
    "\n",
    "#\n",
    "len( diseases_used_for_training ), len( diseases_used_for_test )            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get span, name and types of concepts in brat\n",
    "# dict_of_brat_annotations.setdefault( ent_id, ('FindingSite', ent_name, pos_info) )    \n",
    "def read_brat_annotation_file( file_of_brat_annotation ):\n",
    "    # to get\n",
    "    dict_of_brat_annotations = {}\n",
    "    # text string\n",
    "    text_string = \"\"\n",
    "    with open(file_of_brat_annotation,'r',encoding='utf-8') as f:\n",
    "        text_string = f.read()    \n",
    "    # annotation content\n",
    "    ann_lines = []\n",
    "    with open(file_of_brat_annotation,'r',encoding='utf-8') as f:\n",
    "        ann_lines = f.readlines()\n",
    "    # resolve annotation content line by line\n",
    "    for line in ann_lines:\n",
    "        # \n",
    "        if line.startswith('T'): \n",
    "            # phenotype concept\n",
    "            if re.search('Phenotype',line,re.I):\n",
    "                # T1\tPhenotype 69 86\tpainful abscesses\n",
    "                ent_id, ent_info, ent_name = line.strip().split('\\t')\n",
    "                # position info 153 157;180 194\n",
    "                pos_info = ent_info.replace('Phenotype ','')\n",
    "                # \n",
    "                # dict_of_brat_annotations.setdefault( ent_id, ('Phenotype', ent_name, pos_info) )   \n",
    "                dict_of_brat_annotations.setdefault( pos_info, ('Phenotype', ent_name, ent_id) )\n",
    "            # findingsite concept\n",
    "            elif re.search('FindingSite',line,re.I):\n",
    "                # T4\tFindingSite 114 120\tbreast\n",
    "                ent_id, ent_info, ent_name = line.strip().split('\\t')\n",
    "                # position 114 120\n",
    "                pos_info = ent_info.replace('FindingSite ','')   \n",
    "                #\n",
    "                # dict_of_brat_annotations.setdefault( ent_id, ('FindingSite', ent_name, pos_info) )   \n",
    "                dict_of_brat_annotations.setdefault( pos_info, ('FindingSite', ent_name, ent_id) )     \n",
    "    #\n",
    "    return dict_of_brat_annotations   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_name = \"Aspergillosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve annotation results of metamap , index by span (compatible with brat)\n",
    "# concept in concepts \n",
    "# continuous concept\n",
    "# The simplest form : 228/6  pos_info='228/6' \n",
    "# non-continuous concept\n",
    "# Multiple comma-separated StartPos/Length pairs:  7059/5,7073/5 (indicating disjoint text strings mapped to one concept)\n",
    "# Multiple comma-separated bracketed StartPos/Length pairs:  [1351/8],[1437/8]  multiple occurrences of a concept in an utterance\n",
    "# Finally, forms (b) and (c) above can in rare cases be combined, e.g., [4061/10,4075/11],[4166/10,4180/11] \n",
    "def read_metamap_annotation_result( textstr, list_of_concepts_in_text ):\n",
    "    # to obtain\n",
    "    # span_key (compatible with brat) : concept\n",
    "    metamap_annotated_concepts_by_spans = {}\n",
    "\n",
    "    # sentences and positions\n",
    "    text_sents_with_spans = []\n",
    "    for start, end in PunktSentenceTokenizer().span_tokenize(textstr):\n",
    "        text_sents_with_spans.append( (start,end, textstr[start:end]) )\n",
    "\n",
    "    # iterate concepts\n",
    "    for concept in list_of_concepts_in_text:\n",
    "        span_key = \"\"\n",
    "\n",
    "        # location of sentences in text\n",
    "        sent_idx  = int( concept.index )\n",
    "        sent_spos = text_sents_with_spans[sent_idx][0]\n",
    "\n",
    "        # \n",
    "        string_of_pos_info = \"\"\n",
    "        if ';' in concept.pos_info:\n",
    "            string_of_pos_info = concept.pos_info.split(';')[0]\n",
    "        else:\n",
    "            string_of_pos_info = concept.pos_info\n",
    "\n",
    "        # concept.pos_info \n",
    "        if '[' not in string_of_pos_info:\n",
    "            # case a)\n",
    "            if string_of_pos_info.count('/') == 1 :\n",
    "                # 统计 case a\n",
    "                spos, slen = string_of_pos_info.split('/')\n",
    "                spos = int(spos) -1 + sent_spos\n",
    "                slen = int(slen)\n",
    "                epos = spos + slen\n",
    "                # \n",
    "                span_key = str(spos) + ' '+ str(epos) \n",
    "                # \n",
    "                if span_key not in metamap_annotated_concepts_by_spans:\n",
    "                    metamap_annotated_concepts_by_spans.setdefault( span_key, concept )\n",
    "\n",
    "            #  case b\n",
    "            elif string_of_pos_info.count('/') > 1:\n",
    "                # ‘7059/5,7073/5’ \n",
    "                pos_of_parts = []\n",
    "                for part_pos_info in string_of_pos_info.split(','):\n",
    "                    # print( part_pos_info , concept)\n",
    "                    spos, slen = part_pos_info.split('/')\n",
    "                    spos = int(spos) -1 + sent_spos\n",
    "                    slen = int(slen)\n",
    "                    epos = spos + slen  \n",
    "                    # \n",
    "                    pos_of_parts.append( str(spos) + ' '+ str(epos)  ) \n",
    "                #\n",
    "                span_key = ';'.join( pos_of_parts )  \n",
    "                # \n",
    "                if span_key not in metamap_annotated_concepts_by_spans:\n",
    "                    metamap_annotated_concepts_by_spans.setdefault( span_key, concept )                \n",
    "        # \n",
    "        # [1351/8],[1437/8]  case c)\n",
    "        # [4061/10,4075/11],[4166/10,4180/11]   case d)\n",
    "        else:\n",
    "            #  [1351/8],[1437/8] --> ['[1351/8]', '[1437/8]']\n",
    "            for bracket_pos_info in re.findall('\\[.*?\\]',string_of_pos_info):\n",
    "                # change  '[1351/8]' to '1351/8' ,  case a)\n",
    "                # change  '[4061/10,4075/11]' to 4061/10,4075/11 , case b)\n",
    "                cleaned_pos_info = bracket_pos_info[1:-1]\n",
    "                # \n",
    "                # case c) -->  case a)\n",
    "                if cleaned_pos_info.count('/') == 1 :\n",
    "                    # \n",
    "                    spos, slen = cleaned_pos_info.split('/')\n",
    "                    spos = int(spos) -1 + sent_spos\n",
    "                    slen = int(slen)\n",
    "                    epos = spos + slen\n",
    "                    # \n",
    "                    span_key = str(spos) + ' '+ str(epos) \n",
    "                    # \n",
    "                    if span_key not in metamap_annotated_concepts_by_spans:\n",
    "                        metamap_annotated_concepts_by_spans.setdefault( span_key, concept )                    \n",
    "                # case d) --> case b\n",
    "                elif cleaned_pos_info.count('/') > 1:\n",
    "                    # ‘7059/5,7073/5’ \n",
    "                    pos_of_parts = []\n",
    "                    for part_pos_info in cleaned_pos_info.split(','):\n",
    "                        spos, slen = part_pos_info.split('/')\n",
    "                        spos = int(spos) -1 + sent_spos\n",
    "                        slen = int(slen)\n",
    "                        epos = spos + slen  \n",
    "                        # \n",
    "                        pos_of_parts.append( str(spos) + ' '+ str(epos)  ) \n",
    "                    #\n",
    "                    span_key = ';'.join( pos_of_parts )  \n",
    "                    # \n",
    "                    if span_key not in metamap_annotated_concepts_by_spans:\n",
    "                        metamap_annotated_concepts_by_spans.setdefault( span_key, concept )        \n",
    "    #\n",
    "    return metamap_annotated_concepts_by_spans                                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[4194, 225, 243, 0, 187]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# statistics of entities annotated by metamap\n",
    "\n",
    "counts = [0]*5\n",
    "\n",
    "for disease_name in list_of_diseases:\n",
    "# for disease_name in diseases_used_for_training:\n",
    "    # disease_name的metamap annotation result\n",
    "    concepts_annotated_by_metamap = dict_of_concepts_in_texts[disease_name]\n",
    "    # 将pos_info的形式输出来看看\n",
    "    for concept in concepts_annotated_by_metamap:\n",
    "        string_of_pos_info = \"\"\n",
    "        if ';' in concept.pos_info:\n",
    "            string_of_pos_info = concept.pos_info.split(';')[0]\n",
    "            counts[4] +=1\n",
    "        else:\n",
    "            string_of_pos_info = concept.pos_info\n",
    "\n",
    "        # concept.pos_info \n",
    "        # the simplest form \n",
    "        if '[' not in string_of_pos_info:\n",
    "            #  case a)\n",
    "            if string_of_pos_info.count('/') == 1 :\n",
    "                #  case a\n",
    "                counts[0] +=1\n",
    "                # position\n",
    "                spos, slen = string_of_pos_info.split('/')\n",
    "                spos = int(spos) -1\n",
    "                slen = int(slen)\n",
    "                epos = spos + slen\n",
    "                # \n",
    "                span_key = str(spos) + ' '+ str(epos) \n",
    "            # case b\n",
    "            elif string_of_pos_info.count('/') > 1:\n",
    "                #  case b\n",
    "                counts[1] += 1\n",
    "                # ‘7059/5,7073/5’ \n",
    "                pos_of_parts = []\n",
    "                for part_pos_info in string_of_pos_info.split(','):\n",
    "                    # print( part_pos_info , concept)\n",
    "                    spos, slen = part_pos_info.split('/')\n",
    "                    spos = int(spos) -1\n",
    "                    slen = int(slen)\n",
    "                    epos = spos + slen  \n",
    "                    # \n",
    "                    pos_of_parts.append( str(spos) + ' '+ str(epos)  ) \n",
    "                #\n",
    "                span_key = ';'.join( pos_of_parts )  \n",
    "        # \n",
    "        # [4061/10,4075/11],[4166/10,4180/11]  case d)\n",
    "        else:\n",
    "            # [1351/8],[1437/8] --> ['[1351/8]', '[1437/8]']\n",
    "            for bracket_pos_info in re.findall('\\[.*?\\]',string_of_pos_info):\n",
    "                # 将 '[1351/8]' 变成 '1351/8' ,  case a)\n",
    "                # 将 '[4061/10,4075/11]' 变成 4061/10,4075/11 , case b)\n",
    "                cleaned_pos_info = bracket_pos_info[1:-1]\n",
    "                # \n",
    "                # case c) --> case a)\n",
    "                if cleaned_pos_info.count('/') == 1 :\n",
    "                    # count\n",
    "                    counts[2] +=1\n",
    "                    # 解析概念的起止位置\n",
    "                    spos, slen = cleaned_pos_info.split('/')\n",
    "                    spos = int(spos) -1\n",
    "                    slen = int(slen)\n",
    "                    epos = spos + slen\n",
    "                    # 生成brat中的span_key\n",
    "                    span_key = str(spos) + ' '+ str(epos) \n",
    "                # case d) -->  case b\n",
    "                elif cleaned_pos_info.count('/') > 1:\n",
    "                    # count\n",
    "                    counts[3] += 1\n",
    "                    # ‘7059/5,7073/5’ \n",
    "                    pos_of_parts = []\n",
    "                    for part_pos_info in cleaned_pos_info.split(','):\n",
    "                        spos, slen = part_pos_info.split('/')\n",
    "                        spos = int(spos) -1\n",
    "                        slen = int(slen)\n",
    "                        epos = spos + slen  \n",
    "                        # \n",
    "                        pos_of_parts.append( str(spos) + ' '+ str(epos)  ) \n",
    "                    #\n",
    "                    span_key = ';'.join( pos_of_parts )                 \n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare text span annotated by human and machine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interval import Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# precision = true prediction/total prediction\n",
    "\n",
    "# to obtain\n",
    "full_matched_pred_entity_counts    = [0,0]\n",
    "partial_matched_pred_entity_counts = [0,0]\n",
    "\n",
    "#  \n",
    "total_pred_entity_counts = [0,0]\n",
    "\n",
    "\n",
    "for disease_name in diseases_used_for_training:\n",
    "    # \n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read() \n",
    "    # \n",
    "    raw_text = raw_text.replace('\\n',',')\n",
    "\n",
    "    # human annotation\n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "\n",
    "    # machine annotation\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "\n",
    "    # compare\n",
    "    for span_key_of_metamap in dict_of_meta_annotations:\n",
    "        # metamap concepts\n",
    "        concept = dict_of_meta_annotations[span_key_of_metamap]\n",
    "\n",
    "        # strict and relax match\n",
    "        strict_match_of_interval = False\n",
    "        relax_match_of_interval  = False\n",
    "\n",
    "        # entity type\n",
    "        pred_type = \"\"\n",
    "        # '[sosy]'\n",
    "        if concept.semtypes[1:-1] in semantic_group_of_disorder :\n",
    "            pred_type = 'Phenotype'\n",
    "        elif concept.semtypes[1:-1] in semantic_group_of_anatomy :\n",
    "            pred_type = 'FindingSite'\n",
    "        \n",
    "        # stat\n",
    "        if pred_type == 'Phenotype':\n",
    "            total_pred_entity_counts[0] +=1\n",
    "        elif pred_type == 'FindingSite':\n",
    "            total_pred_entity_counts[1] +=1\n",
    "\n",
    "        # span\n",
    "        list_of_span_intervals_by_metamap = []\n",
    "\n",
    "        # \n",
    "        if ';' in span_key_of_metamap:\n",
    "            for pos_info in span_key_of_metamap.split(';'):\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # \n",
    "                list_of_span_intervals_by_metamap.append( Interval(spos, epos) )\n",
    "        else:\n",
    "            pos_info = span_key_of_metamap\n",
    "            #\n",
    "            spos, epos = pos_info.split(' ')\n",
    "            spos = int(spos)\n",
    "            epos = int(epos)\n",
    "            # \n",
    "            list_of_span_intervals_by_metamap.append( Interval(spos, epos) )     \n",
    "\n",
    "        # list_of_span_intervals_by_metamap\n",
    "        # compare\n",
    "        for span_key_of_expert in dict_of_brat_annotations:\n",
    "            # type\n",
    "            real_type = dict_of_brat_annotations[span_key_of_expert][0] \n",
    "\n",
    "            # span\n",
    "            list_of_span_intervals_by_expert = []\n",
    "\n",
    "            # span_key\n",
    "            if ';' in span_key_of_expert:\n",
    "                #  span_key_of_expert mistaked as span_key_of_metamap\n",
    "                for pos_info in span_key_of_expert.split(';'):\n",
    "                    spos, epos = pos_info.split(' ')\n",
    "                    spos = int(spos)\n",
    "                    epos = int(epos)\n",
    "                    # \n",
    "                    list_of_span_intervals_by_expert.append( Interval(spos, epos) )\n",
    "            else:\n",
    "                pos_info = span_key_of_expert\n",
    "                #\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # \n",
    "                list_of_span_intervals_by_expert.append( Interval(spos, epos) )      \n",
    "\n",
    "            # compare\n",
    "            if real_type == pred_type:\n",
    "                if set( list_of_span_intervals_by_expert ) == set( list_of_span_intervals_by_metamap ):\n",
    "                    # print(\"full match\", list_of_span_intervals_by_expert, list_of_span_intervals_by_metamap )\n",
    "                    strict_match_of_interval = True\n",
    "                    break\n",
    "                else:\n",
    "                    # overlap\n",
    "                    for _span_interval_by_metamap in list_of_span_intervals_by_metamap:\n",
    "                        if relax_match_of_interval == True:\n",
    "                            break\n",
    "                        else:\n",
    "                            for _span_interval_by_expert in list_of_span_intervals_by_expert:\n",
    "                                if _span_interval_by_metamap.overlaps( _span_interval_by_expert ):\n",
    "                                    relax_match_of_interval = True\n",
    "                                    break\n",
    "\n",
    "        # count\n",
    "        if strict_match_of_interval == True:\n",
    "            # print( \"full match\" +'\\t' + span_key_of_metamap +'\\t' + concept.trigger + '\\t' )\n",
    "            if pred_type == 'Phenotype':\n",
    "                full_matched_pred_entity_counts[0] +=1\n",
    "            elif pred_type == 'FindingSite':\n",
    "                full_matched_pred_entity_counts[1] +=1      \n",
    "        elif relax_match_of_interval == True:\n",
    "            # print( \"partial match\" +'\\t' + span_key_of_metamap +'\\t' + concept.trigger + '\\t' )\n",
    "            if pred_type == 'Phenotype':\n",
    "                partial_matched_pred_entity_counts[0] +=1\n",
    "            elif pred_type == 'FindingSite':\n",
    "                partial_matched_pred_entity_counts[1] +=1                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1696, 0]\n[475, 0]\n[3341, 0]\n"
    }
   ],
   "source": [
    "# print\n",
    "print(full_matched_pred_entity_counts)\n",
    "print(partial_matched_pred_entity_counts)\n",
    "print(total_pred_entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall = coverred annotations/total annotations\n",
    "\n",
    "# to get\n",
    "full_coverred_real_entity_counts    = [0,0]\n",
    "partial_coverred_real_entity_counts = [0,0]\n",
    "\n",
    "#  \n",
    "total_real_entity_counts = [0,0]\n",
    "\n",
    "for disease_name in diseases_used_for_training:\n",
    "    # \n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read() \n",
    "    raw_text = raw_text.replace('\\n',',') \n",
    "\n",
    "    # human annotation\n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "\n",
    "    # machine annotation\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "\n",
    "\n",
    "    # compare\n",
    "    for span_key_of_expert in dict_of_brat_annotations:\n",
    "        # \n",
    "        strict_match_of_interval = False\n",
    "        relax_match_of_interval  = False\n",
    "\n",
    "        # type\n",
    "        real_type = dict_of_brat_annotations[span_key_of_expert][0] \n",
    "\n",
    "        # count\n",
    "        if real_type == 'Phenotype':\n",
    "            total_real_entity_counts[0] +=1\n",
    "        elif real_type == 'FindingSite':\n",
    "            total_real_entity_counts[1] +=1\n",
    "\n",
    "        # span\n",
    "        list_of_span_intervals_by_expert = []\n",
    "\n",
    "        # span_key\n",
    "        if ';' in span_key_of_expert:\n",
    "            for pos_info in span_key_of_expert.split(';'):\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                list_of_span_intervals_by_expert.append( Interval(spos, epos) )\n",
    "        else:\n",
    "            pos_info = span_key_of_expert\n",
    "            #\n",
    "            spos, epos = pos_info.split(' ')\n",
    "            spos = int(spos)\n",
    "            epos = int(epos)\n",
    "            list_of_span_intervals_by_expert.append( Interval(spos, epos) )          \n",
    "\n",
    "        # compare\n",
    "        for span_key_of_metamap in dict_of_meta_annotations:\n",
    "            # metamap concept\n",
    "            concept = dict_of_meta_annotations[span_key_of_metamap]\n",
    "\n",
    "            # type\n",
    "            pred_type = \"\"\n",
    "            # '[sosy]'\n",
    "            if concept.semtypes[1:-1] in semantic_group_of_disorder :\n",
    "                pred_type = 'Phenotype'\n",
    "            elif concept.semtypes[1:-1] in semantic_group_of_anatomy :\n",
    "                pred_type = 'FindingSite'\n",
    "            \n",
    "\n",
    "            # \n",
    "            list_of_span_intervals_by_metamap = []\n",
    "\n",
    "            # \n",
    "            if ';' in span_key_of_metamap:\n",
    "                for pos_info in span_key_of_metamap.split(';'):\n",
    "                    spos, epos = pos_info.split(' ')\n",
    "                    spos = int(spos)\n",
    "                    epos = int(epos)\n",
    "                    # \n",
    "                    list_of_span_intervals_by_metamap.append( Interval(spos, epos) )\n",
    "            else:\n",
    "                pos_info = span_key_of_metamap\n",
    "                #\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # \n",
    "                list_of_span_intervals_by_metamap.append( Interval(spos, epos) )    \n",
    "\n",
    "            # list_of_span_intervals_by_metamap \n",
    "            if real_type == pred_type:\n",
    "                # \n",
    "                if set( list_of_span_intervals_by_expert ) == set( list_of_span_intervals_by_metamap ):\n",
    "                    strict_match_of_interval = True\n",
    "                    break\n",
    "                # \n",
    "                else:\n",
    "                    # \n",
    "                    for _span_interval_by_expert in list_of_span_intervals_by_expert:\n",
    "                        if relax_match_of_interval == True:\n",
    "                            break\n",
    "                        else:\n",
    "                            for _span_interval_by_metamap in list_of_span_intervals_by_metamap:\n",
    "                                if _span_interval_by_expert.overlaps( _span_interval_by_metamap ):\n",
    "                                    relax_match_of_interval = True\n",
    "                                    break   \n",
    "\n",
    "        # strict_match_of_interval                           \n",
    "        if strict_match_of_interval == True:\n",
    "            if real_type == 'Phenotype':\n",
    "                full_coverred_real_entity_counts[0] +=1\n",
    "            elif real_type == 'FindingSite':\n",
    "                full_coverred_real_entity_counts[1] +=1      \n",
    "        elif relax_match_of_interval == True:\n",
    "            if real_type == 'Phenotype':\n",
    "                partial_coverred_real_entity_counts[0] +=1\n",
    "            elif real_type == 'FindingSite':\n",
    "                partial_coverred_real_entity_counts[1] +=1                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1696, 0]\n[481, 0]\n[2917, 332]\n"
    }
   ],
   "source": [
    "print(full_coverred_real_entity_counts)\n",
    "print(partial_coverred_real_entity_counts)\n",
    "print(total_real_entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filting General but meanless terms\n",
    "# CUI : count\n",
    "stat_of_concepts = {}\n",
    "\n",
    "for disease_name in diseases_used_for_training:\n",
    "    # \n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read()  \n",
    "    raw_text = raw_text.replace('\\n',',')\n",
    "\n",
    "    # \n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "\n",
    "    # machine annotation\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "\n",
    "    # compare\n",
    "    for span_key_of_metamap in dict_of_meta_annotations:\n",
    "        # \n",
    "        concept = dict_of_meta_annotations[span_key_of_metamap]\n",
    "        #\n",
    "        # if not concept.cui.startswith('C'):\n",
    "        #     print( concept )        \n",
    "        # \n",
    "        if concept.cui + '::' + concept.preferred_name not in stat_of_concepts:\n",
    "            stat_of_concepts.setdefault( concept.cui + '::' + concept.preferred_name, 1 )\n",
    "        else:\n",
    "            stat_of_concepts[ concept.cui + '::' + concept.preferred_name ] += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "602"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# count\n",
    "len(stat_of_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat_of_concepts\n",
    "occurrences_of_concepts = []\n",
    "\n",
    "for key in stat_of_concepts:\n",
    "    count = stat_of_concepts[key]\n",
    "    occurrences_of_concepts.append( (count, key) )\n",
    "\n",
    "occurrences_of_concepts.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(222, 'C0015967::Fever'),\n (117, 'C0015230::Exanthema'),\n (114, 'C0205082::Severe (severity modifier)'),\n (95, 'C0231218::Malaise'),\n (85, 'C0032285::Pneumonia'),\n (85, 'C0030193::Pain'),\n (73, 'C0018681::Headache'),\n (64, 'C0011991::Diarrhea'),\n (58, 'C0010200::Coughing'),\n (52, 'C0042963::Vomiting')]"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# occurrences_of_concepts[0:100]\n",
    "occurrences_of_concepts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_excluded_cuis = []\n",
    "\n",
    "with open(\"/home/denglizong/SSUMiner/corpus/Excluded_Concepts/excluded_cuis.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if len( line.strip().split('\\t') ) == 2:\n",
    "            excluded_cui, excluded_name = line.strip().split('\\t')\n",
    "            list_of_excluded_cuis.append( excluded_cui )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "344"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "len( set(list_of_excluded_cuis) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['C5203670', 'C5203340', 'C5203106']"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "list_of_excluded_cuis[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean metamap annotation\n",
    "def clean_metamap_annotations_in_text( raw_metamap_annotations, list_of_excluded_cuis, disorder_tags, anatomy_tags ):\n",
    "    # \n",
    "    span_keys_to_be_excluded = []\n",
    "\n",
    "    # \n",
    "    counts_of_remove = [0]*3\n",
    "\n",
    "    # sent_index : [ (span_key, concept) ]\n",
    "    sent_level_annotations = {}\n",
    "\n",
    "    for span_key in raw_metamap_annotations:\n",
    "        # \n",
    "        concept = raw_metamap_annotations[span_key]\n",
    "\n",
    "        # rule 1: NN\n",
    "        if '-noun-' not in concept.trigger:\n",
    "            span_keys_to_be_excluded.append( span_key )\n",
    "            counts_of_remove[0] += 1\n",
    "        else:\n",
    "            # rule 2：remove concepts without specific meanings\n",
    "             if concept.cui in list_of_excluded_cuis:\n",
    "                 span_keys_to_be_excluded.append( span_key )\n",
    "                 counts_of_remove[1] += 1\n",
    "        \n",
    "        #\n",
    "        if concept.index not in sent_level_annotations:\n",
    "            sent_level_annotations.setdefault( concept.index, [(span_key, concept)] )\n",
    "        else:\n",
    "            sent_level_annotations[concept.index].append( (span_key, concept) )\n",
    "    \n",
    "    #\n",
    "    for sent_idx in sent_level_annotations:\n",
    "        # observe finding sites\n",
    "        phenotype_in_sent = False\n",
    "        findingsite_in_sent = False\n",
    "\n",
    "        for (span_key, concept) in sent_level_annotations[sent_idx]:\n",
    "            if concept.semtypes[1:-1] in disorder_tags:\n",
    "                phenotype_in_sent = True\n",
    "            elif concept.semtypes[1:-1] in anatomy_tags:\n",
    "                findingsite_in_sent = True\n",
    "        \n",
    "        # \n",
    "        if phenotype_in_sent == False and findingsite_in_sent == True:\n",
    "            for (span_key, concept) in sent_level_annotations[sent_idx]:\n",
    "                if concept.semtypes[1:-1] in anatomy_tags:\n",
    "                    if span_key not in span_keys_to_be_excluded:\n",
    "                        span_keys_to_be_excluded.append( span_key )  \n",
    "                        counts_of_remove[2] += 1\n",
    "\n",
    "    # \n",
    "    clean_metamap_annotations = {}\n",
    "\n",
    "    for span_key in raw_metamap_annotations:\n",
    "        # \n",
    "        concept = raw_metamap_annotations[span_key]\n",
    "        #\n",
    "        if span_key not in span_keys_to_be_excluded:\n",
    "            clean_metamap_annotations.setdefault( span_key, concept )\n",
    "    \n",
    "    #\n",
    "    # print( counts_of_remove )\n",
    "    #\n",
    "    return clean_metamap_annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean metamap annotation with hpo\n",
    "def clean_metamap_annotations_with_hpo( raw_metamap_annotations, list_of_excluded_cuis, disorder_tags, anatomy_tags ):\n",
    "    # \n",
    "    span_keys_to_be_excluded = []\n",
    "\n",
    "    #\n",
    "\n",
    "    for span_key in raw_metamap_annotations:\n",
    "        # \n",
    "        concept = raw_metamap_annotations[span_key]\n",
    "\n",
    "        # rule 1：nn\n",
    "        # if '-noun-' not in concept.trigger:\n",
    "        #     span_keys_to_be_excluded.append( span_key )\n",
    "        #     counts_of_remove[0] += 1\n",
    "        # else:\n",
    "        #     # rule 2：remove concepts without specific meanings\n",
    "        #      if concept.cui in list_of_excluded_cuis:\n",
    "        #          span_keys_to_be_excluded.append( span_key )\n",
    "        #          counts_of_remove[1] += 1\n",
    "\n",
    "        # \n",
    "        if concept.cui in list_of_excluded_cuis:\n",
    "            span_keys_to_be_excluded.append( span_key )\n",
    "        # \n",
    "\n",
    "    # \n",
    "    clean_metamap_annotations = {}\n",
    "\n",
    "    for span_key in raw_metamap_annotations:\n",
    "        # \n",
    "        concept = raw_metamap_annotations[span_key]\n",
    "        #\n",
    "        if span_key not in span_keys_to_be_excluded:\n",
    "            clean_metamap_annotations.setdefault( span_key, concept )\n",
    "    \n",
    "    #\n",
    "    # print( counts_of_remove )\n",
    "    #\n",
    "    return clean_metamap_annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HPO表型异常对应的CUI\n",
    "list_of_excluded_cuis_in_hpo = []\n",
    "\n",
    "with open(\"/home/denglizong/SSUMiner/corpus/Excluded_Concepts/excluded_cuis_in_hpo.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.startswith('C'):\n",
    "            list_of_excluded_cuis_in_hpo.append( line.strip() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "101"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "len(list_of_excluded_cuis_in_hpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['C1708511', 'C0443147', 'C4020899']"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "list_of_excluded_cuis_in_hpo[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-calculate precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对于每一份疾病描述文本\n",
    "# 观察机器所做的预测，并基于人工预测评估机器所作的预测是否正确 \n",
    "# precision = true prediction/total prediction\n",
    "\n",
    "# 预测正确的表型实体或部位实体\n",
    "full_matched_pred_entity_counts    = [0,0]\n",
    "partial_matched_pred_entity_counts = [0,0]\n",
    "\n",
    "# 所有预测 \n",
    "total_pred_entity_counts = [0,0]\n",
    "\n",
    "\n",
    "# for disease_name in diseases_used_for_training:\n",
    "for disease_name in diseases_used_for_test:\n",
    "    # 疾病百科原文\n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read() \n",
    "    raw_text = raw_text.replace('\\n',',') \n",
    "\n",
    "    # 获取该疾病文本的人工标注\n",
    "    # 原来是这里出错了，将 disease_name 设置为了 \"Acinetobacter infections\"\n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "\n",
    "    # 获取该疾病文本的机器标注\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "    # dict_of_meta_annotations_cleaned = clean_metamap_annotations_in_text( dict_of_meta_annotations, \\\n",
    "    #                                         list_of_excluded_cuis, semantic_group_of_disorder, semantic_group_of_anatomy )\n",
    "    dict_of_meta_annotations_cleaned = clean_metamap_annotations_with_hpo( dict_of_meta_annotations, \\\n",
    "                                            list_of_excluded_cuis_in_hpo, semantic_group_of_disorder, semantic_group_of_anatomy )    \n",
    "\n",
    "    # 比较机器标注与人工标注\n",
    "    for span_key_of_metamap in dict_of_meta_annotations_cleaned:\n",
    "        # metamap注释的概念\n",
    "        concept = dict_of_meta_annotations_cleaned[span_key_of_metamap]\n",
    "\n",
    "        # 是否存在一个人工标注，与metamap的标注完全一致或部分一致 \n",
    "        # 实体类型一样的前提下，区间标注完全一致或部分一致(重叠)\n",
    "        strict_match_of_interval = False\n",
    "        relax_match_of_interval  = False\n",
    "\n",
    "        # 该区间概念对应的实体类型\n",
    "        pred_type = \"\"\n",
    "        # '[sosy]'\n",
    "        if concept.semtypes[1:-1] in semantic_group_of_disorder :\n",
    "            pred_type = 'Phenotype'\n",
    "        elif concept.semtypes[1:-1] in semantic_group_of_anatomy :\n",
    "            pred_type = 'FindingSite'\n",
    "        \n",
    "        # 统计预测\n",
    "        if pred_type == 'Phenotype':\n",
    "            total_pred_entity_counts[0] +=1\n",
    "        elif pred_type == 'FindingSite':\n",
    "            total_pred_entity_counts[1] +=1\n",
    "\n",
    "        # 生成区间；由于不连续实体的存在，可能有多段区间\n",
    "        list_of_span_intervals_by_metamap = []\n",
    "\n",
    "        # \n",
    "        if ';' in span_key_of_metamap:\n",
    "            for pos_info in span_key_of_metamap.split(';'):\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # 生成span区间，便于比较\n",
    "                list_of_span_intervals_by_metamap.append( Interval(spos, epos) )\n",
    "        else:\n",
    "            pos_info = span_key_of_metamap\n",
    "            #\n",
    "            spos, epos = pos_info.split(' ')\n",
    "            spos = int(spos)\n",
    "            epos = int(epos)\n",
    "            # 生成span区间，便于比较\n",
    "            list_of_span_intervals_by_metamap.append( Interval(spos, epos) )     \n",
    "\n",
    "        # list_of_span_intervals_by_metamap\n",
    "        # 观察这一区间是否存在于人工标注的区间中 (且实体类型一致)\n",
    "        for span_key_of_expert in dict_of_brat_annotations:\n",
    "            # 实体类型\n",
    "            real_type = dict_of_brat_annotations[span_key_of_expert][0] \n",
    "\n",
    "            # span区间化\n",
    "            list_of_span_intervals_by_expert = []\n",
    "\n",
    "            # 解析span_key\n",
    "            if ';' in span_key_of_expert:\n",
    "                # 这里出错啦 span_key_of_expert mistaked as span_key_of_metamap\n",
    "                for pos_info in span_key_of_expert.split(';'):\n",
    "                    spos, epos = pos_info.split(' ')\n",
    "                    spos = int(spos)\n",
    "                    epos = int(epos)\n",
    "                    # 生成span区间，便于比较\n",
    "                    list_of_span_intervals_by_expert.append( Interval(spos, epos) )\n",
    "            else:\n",
    "                pos_info = span_key_of_expert\n",
    "                #\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # 生成span区间，便于比较\n",
    "                list_of_span_intervals_by_expert.append( Interval(spos, epos) )      \n",
    "\n",
    "            # 在实体类型一致的基础上，进行区间一致性比较 \n",
    "            if real_type == pred_type:\n",
    "                # 区间一致性比较\n",
    "                # 完全一致\n",
    "                if set( list_of_span_intervals_by_expert ) == set( list_of_span_intervals_by_metamap ):\n",
    "                    # print(\"full match\", list_of_span_intervals_by_expert, list_of_span_intervals_by_metamap )\n",
    "                    strict_match_of_interval = True\n",
    "                    break\n",
    "                # 如果两者区间不相等\n",
    "                else:\n",
    "                    # 但如果存在重叠关系，设定relax_match = True \n",
    "                    for _span_interval_by_metamap in list_of_span_intervals_by_metamap:\n",
    "                        if relax_match_of_interval == True:\n",
    "                            break\n",
    "                        else:\n",
    "                            for _span_interval_by_expert in list_of_span_intervals_by_expert:\n",
    "                                if _span_interval_by_metamap.overlaps( _span_interval_by_expert ):\n",
    "                                    relax_match_of_interval = True\n",
    "                                    break\n",
    "\n",
    "        # 计数完全一致的预测和部分一致的预测\n",
    "        # 并根据类型分别记录表型和部位实体预测的一致性\n",
    "        if strict_match_of_interval == True:\n",
    "            # print( \"full match\" +'\\t' + span_key_of_metamap +'\\t' + concept.trigger + '\\t' )\n",
    "            if pred_type == 'Phenotype':\n",
    "                full_matched_pred_entity_counts[0] +=1\n",
    "            elif pred_type == 'FindingSite':\n",
    "                full_matched_pred_entity_counts[1] +=1      \n",
    "        elif relax_match_of_interval == True:\n",
    "            # print( \"partial match\" +'\\t' + span_key_of_metamap +'\\t' + concept.trigger + '\\t' )\n",
    "            if pred_type == 'Phenotype':\n",
    "                partial_matched_pred_entity_counts[0] +=1\n",
    "            elif pred_type == 'FindingSite':\n",
    "                partial_matched_pred_entity_counts[1] +=1                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[631, 0]\n[171, 0]\n[1266, 0]\n"
    }
   ],
   "source": [
    "# 预测的准确率(冗余)较大，是需要通过规则进行过滤\n",
    "print(full_matched_pred_entity_counts)\n",
    "print(partial_matched_pred_entity_counts)\n",
    "print(total_pred_entity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计人工标注的概念能被机器标注所覆盖的情况\n",
    "# 对于每一份疾病描述文本\n",
    "# 观察人工标注的概念，并观察人工标注的概念是否能被机器标注所覆盖\n",
    "# recall = coverred annotations/total annotations\n",
    "\n",
    "# 预测正确的表型实体或部位实体\n",
    "full_coverred_real_entity_counts    = [0,0]\n",
    "partial_coverred_real_entity_counts = [0,0]\n",
    "\n",
    "# 所有预测 \n",
    "total_real_entity_counts = [0,0]\n",
    "\n",
    "# for disease_name in diseases_used_for_training:\n",
    "for disease_name in diseases_used_for_test:\n",
    "    # 疾病百科原文\n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read()  \n",
    "    raw_text = raw_text.replace('\\n',',')\n",
    "\n",
    "    # 获取该疾病文本的人工标注\n",
    "    # 原来是这里出错了，将 disease_name 设置为了 \"Acinetobacter infections\"\n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "\n",
    "    # 获取该疾病文本的机器标注\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "    # dict_of_meta_annotations_cleaned = clean_metamap_annotations_in_text( dict_of_meta_annotations, \\\n",
    "    #                                         list_of_excluded_cuis, semantic_group_of_disorder, semantic_group_of_anatomy )\n",
    "    dict_of_meta_annotations_cleaned = clean_metamap_annotations_with_hpo( dict_of_meta_annotations, \\\n",
    "                                            list_of_excluded_cuis_in_hpo, semantic_group_of_disorder, semantic_group_of_anatomy )        \n",
    "\n",
    "    # 比较机器标注与人工标注\n",
    "    # 对于一个人工标注\n",
    "    for span_key_of_expert in dict_of_brat_annotations:\n",
    "        # 是否存在一个机器标注，能完全或部分的覆盖该人工标注？\n",
    "        strict_match_of_interval = False\n",
    "        relax_match_of_interval  = False\n",
    "\n",
    "        # 实体类型\n",
    "        real_type = dict_of_brat_annotations[span_key_of_expert][0] \n",
    "\n",
    "        # 统计标注\n",
    "        if real_type == 'Phenotype':\n",
    "            total_real_entity_counts[0] +=1\n",
    "        elif real_type == 'FindingSite':\n",
    "            total_real_entity_counts[1] +=1\n",
    "\n",
    "        # span区间化\n",
    "        list_of_span_intervals_by_expert = []\n",
    "\n",
    "        # 解析span_key\n",
    "        if ';' in span_key_of_expert:\n",
    "            # 这里出错啦，span_key_of_expert mistaken for span_key_of_metamap\n",
    "            for pos_info in span_key_of_expert.split(';'):\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # 生成span区间，便于比较\n",
    "                list_of_span_intervals_by_expert.append( Interval(spos, epos) )\n",
    "        else:\n",
    "            pos_info = span_key_of_expert\n",
    "            #\n",
    "            spos, epos = pos_info.split(' ')\n",
    "            spos = int(spos)\n",
    "            epos = int(epos)\n",
    "            # 生成span区间，便于比较\n",
    "            list_of_span_intervals_by_expert.append( Interval(spos, epos) )          \n",
    "\n",
    "        # 看机器标注的区间能否覆盖 list_of_span_intervals_by_expert = []\n",
    "        for span_key_of_metamap in dict_of_meta_annotations_cleaned:\n",
    "            # metamap注释的概念\n",
    "            concept = dict_of_meta_annotations_cleaned[span_key_of_metamap]\n",
    "\n",
    "            # 该区间概念对应的实体类型\n",
    "            pred_type = \"\"\n",
    "            # '[sosy]'\n",
    "            if concept.semtypes[1:-1] in semantic_group_of_disorder :\n",
    "                pred_type = 'Phenotype'\n",
    "            elif concept.semtypes[1:-1] in semantic_group_of_anatomy :\n",
    "                pred_type = 'FindingSite'\n",
    "            \n",
    "\n",
    "            # 生成区间；由于不连续实体的存在，可能有多段区间\n",
    "            list_of_span_intervals_by_metamap = []\n",
    "\n",
    "            # \n",
    "            if ';' in span_key_of_metamap:\n",
    "                for pos_info in span_key_of_metamap.split(';'):\n",
    "                    spos, epos = pos_info.split(' ')\n",
    "                    spos = int(spos)\n",
    "                    epos = int(epos)\n",
    "                    # 生成span区间，便于比较\n",
    "                    list_of_span_intervals_by_metamap.append( Interval(spos, epos) )\n",
    "            else:\n",
    "                pos_info = span_key_of_metamap\n",
    "                #\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # 生成span区间，便于比较\n",
    "                list_of_span_intervals_by_metamap.append( Interval(spos, epos) )    \n",
    "\n",
    "            # list_of_span_intervals_by_metamap \n",
    "            # 在实体类型一致的基础上，进行区间一致性比较 \n",
    "            if real_type == pred_type:\n",
    "                # 区间一致性比较\n",
    "                # 完全一致\n",
    "                if set( list_of_span_intervals_by_expert ) == set( list_of_span_intervals_by_metamap ):\n",
    "                    strict_match_of_interval = True\n",
    "                    break\n",
    "                # 如果两者区间不相等\n",
    "                else:\n",
    "                    # 但如果存在重叠关系，设定relax_match = True \n",
    "                    for _span_interval_by_expert in list_of_span_intervals_by_expert:\n",
    "                        if relax_match_of_interval == True:\n",
    "                            break\n",
    "                        else:\n",
    "                            for _span_interval_by_metamap in list_of_span_intervals_by_metamap:\n",
    "                                if _span_interval_by_expert.overlaps( _span_interval_by_metamap ):\n",
    "                                    relax_match_of_interval = True\n",
    "                                    break   \n",
    "\n",
    "        # strict_match_of_interval                           \n",
    "        # 计数完全一致的预测和部分一致的预测\n",
    "        # 并根据类型分别记录表型和部位实体预测的一致性\n",
    "        if strict_match_of_interval == True:\n",
    "            if real_type == 'Phenotype':\n",
    "                full_coverred_real_entity_counts[0] +=1\n",
    "            elif real_type == 'FindingSite':\n",
    "                full_coverred_real_entity_counts[1] +=1      \n",
    "        elif relax_match_of_interval == True:\n",
    "            if real_type == 'Phenotype':\n",
    "                partial_coverred_real_entity_counts[0] +=1\n",
    "            elif real_type == 'FindingSite':\n",
    "                partial_coverred_real_entity_counts[1] +=1                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[631, 0]\n[182, 0]\n[1104, 126]\n"
    }
   ],
   "source": [
    "print(full_coverred_real_entity_counts)\n",
    "print(partial_coverred_real_entity_counts)\n",
    "print(total_real_entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于在训练集中标注过的术语都算是已知的数据，如果将训练集中的数据加入到MetaMap未识别到的部分 (string-based Method)，观测测试集所能达到的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 统计在训练集中出现过的表型术语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_of_brat_annotations.setdefault( pos_info, ('FindingSite', ent_name, ent_id) )  \n",
    "# 只收集表型术语\n",
    "terms_occurred_in_training_set = set()\n",
    "\n",
    "for disease_name in diseases_used_for_training:\n",
    "    # 获取该疾病文本的人工标注\n",
    "    # 原来是这里出错了，将 disease_name 设置为了 \"Acinetobacter infections\"\n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "    #\n",
    "    for span_key in dict_of_brat_annotations:\n",
    "        ent_type = dict_of_brat_annotations[span_key][0]\n",
    "        ent_name = dict_of_brat_annotations[span_key][1]\n",
    "        if ent_type == 'Phenotype':\n",
    "            terms_occurred_in_training_set.add( ent_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1658"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "len(terms_occurred_in_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['diarrhea with blood',\n 'Congenital rubella syndrome',\n 'collection of fluid in the heart covering',\n 'eroding lesions',\n 'Bleeding from mucous membranes',\n 'disturbances in taste']"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "list(terms_occurred_in_training_set)[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是MetaMap的预测\n",
    "# MetaMap没有覆盖到的人工标注，检查它是否是一个出现过的术语，这一部分也要纳入统计\n",
    "# 用列表记录MetaMap未覆盖的术语 [修改覆盖度计算代码]\n",
    "terms_not_covered_by_MetaMap = []\n",
    "\n",
    "for disease_name in diseases_used_for_test:\n",
    "    # 疾病百科原文\n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read()  \n",
    "    raw_text = raw_text.replace('\\n',',')\n",
    "\n",
    "    # 获取该疾病文本的人工标注\n",
    "    # 原来是这里出错了，将 disease_name 设置为了 \"Acinetobacter infections\"\n",
    "    file_of_brat_annotation  = os.path.join( corpus_path, disease_name + '.ann' )\n",
    "    dict_of_brat_annotations = read_brat_annotation_file(file_of_brat_annotation) \n",
    "\n",
    "    # 获取该疾病文本的机器标注\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "    # dict_of_meta_annotations_cleaned = clean_metamap_annotations_in_text( dict_of_meta_annotations, \\\n",
    "    #                                         list_of_excluded_cuis, semantic_group_of_disorder, semantic_group_of_anatomy )\n",
    "    dict_of_meta_annotations_cleaned = clean_metamap_annotations_with_hpo( dict_of_meta_annotations, \\\n",
    "                                            list_of_excluded_cuis_in_hpo, semantic_group_of_disorder, semantic_group_of_anatomy )        \n",
    "\n",
    "    # 比较机器标注与人工标注\n",
    "    # 对于一个人工标注\n",
    "    for span_key_of_expert in dict_of_brat_annotations:\n",
    "        # 是否存在一个机器标注，能完全或部分的覆盖该人工标注？\n",
    "        strict_match_of_interval = False\n",
    "        relax_match_of_interval  = False\n",
    "\n",
    "        # 实体类型 ent_type\n",
    "        real_type = dict_of_brat_annotations[span_key_of_expert][0] \n",
    "        # 实体名称 ent_name\n",
    "        real_name = dict_of_brat_annotations[span_key_of_expert][1] \n",
    "\n",
    "        # 统计标注\n",
    "        if real_type == 'Phenotype':\n",
    "            total_real_entity_counts[0] +=1\n",
    "        elif real_type == 'FindingSite':\n",
    "            total_real_entity_counts[1] +=1\n",
    "\n",
    "        # span区间化\n",
    "        list_of_span_intervals_by_expert = []\n",
    "\n",
    "        # 解析span_key\n",
    "        if ';' in span_key_of_expert:\n",
    "            # 这里出错啦，span_key_of_expert mistaken for span_key_of_metamap\n",
    "            for pos_info in span_key_of_expert.split(';'):\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # 生成span区间，便于比较\n",
    "                list_of_span_intervals_by_expert.append( Interval(spos, epos) )\n",
    "        else:\n",
    "            pos_info = span_key_of_expert\n",
    "            #\n",
    "            spos, epos = pos_info.split(' ')\n",
    "            spos = int(spos)\n",
    "            epos = int(epos)\n",
    "            # 生成span区间，便于比较\n",
    "            list_of_span_intervals_by_expert.append( Interval(spos, epos) )          \n",
    "\n",
    "        # 看机器标注的区间能否覆盖 list_of_span_intervals_by_expert = []\n",
    "        for span_key_of_metamap in dict_of_meta_annotations_cleaned:\n",
    "            # metamap注释的概念\n",
    "            concept = dict_of_meta_annotations_cleaned[span_key_of_metamap]\n",
    "\n",
    "            # 该区间概念对应的实体类型\n",
    "            pred_type = \"\"\n",
    "            # '[sosy]'\n",
    "            if concept.semtypes[1:-1] in semantic_group_of_disorder :\n",
    "                pred_type = 'Phenotype'\n",
    "            elif concept.semtypes[1:-1] in semantic_group_of_anatomy :\n",
    "                pred_type = 'FindingSite'\n",
    "            \n",
    "\n",
    "            # 生成区间；由于不连续实体的存在，可能有多段区间\n",
    "            list_of_span_intervals_by_metamap = []\n",
    "\n",
    "            # \n",
    "            if ';' in span_key_of_metamap:\n",
    "                for pos_info in span_key_of_metamap.split(';'):\n",
    "                    spos, epos = pos_info.split(' ')\n",
    "                    spos = int(spos)\n",
    "                    epos = int(epos)\n",
    "                    # 生成span区间，便于比较\n",
    "                    list_of_span_intervals_by_metamap.append( Interval(spos, epos) )\n",
    "            else:\n",
    "                pos_info = span_key_of_metamap\n",
    "                #\n",
    "                spos, epos = pos_info.split(' ')\n",
    "                spos = int(spos)\n",
    "                epos = int(epos)\n",
    "                # 生成span区间，便于比较\n",
    "                list_of_span_intervals_by_metamap.append( Interval(spos, epos) )    \n",
    "\n",
    "            # list_of_span_intervals_by_metamap \n",
    "            # 在实体类型一致的基础上，进行区间一致性比较 \n",
    "            if real_type == pred_type:\n",
    "                # 区间一致性比较\n",
    "                # 完全一致\n",
    "                if set( list_of_span_intervals_by_expert ) == set( list_of_span_intervals_by_metamap ):\n",
    "                    strict_match_of_interval = True\n",
    "                    break\n",
    "                # 如果两者区间不相等\n",
    "                else:\n",
    "                    # 但如果存在重叠关系，设定relax_match = True \n",
    "                    for _span_interval_by_expert in list_of_span_intervals_by_expert:\n",
    "                        if relax_match_of_interval == True:\n",
    "                            break\n",
    "                        else:\n",
    "                            for _span_interval_by_metamap in list_of_span_intervals_by_metamap:\n",
    "                                if _span_interval_by_expert.overlaps( _span_interval_by_metamap ):\n",
    "                                    relax_match_of_interval = True\n",
    "                                    break   \n",
    "\n",
    "        # strict_match_of_interval                           \n",
    "        # 计数完全一致的预测和部分一致的预测\n",
    "        # 并根据类型分别记录表型和部位实体预测的一致性\n",
    "        if strict_match_of_interval == True:\n",
    "            if real_type == 'Phenotype':\n",
    "                full_coverred_real_entity_counts[0] +=1\n",
    "            elif real_type == 'FindingSite':\n",
    "                full_coverred_real_entity_counts[1] +=1      \n",
    "        elif relax_match_of_interval == True:\n",
    "            if real_type == 'Phenotype':\n",
    "                partial_coverred_real_entity_counts[0] +=1\n",
    "            elif real_type == 'FindingSite':\n",
    "                partial_coverred_real_entity_counts[1] +=1 \n",
    "\n",
    "        # 如果该术语没有被MetaMap覆盖，记录这一未被MetaMap覆盖的实体\n",
    "        if strict_match_of_interval == False and relax_match_of_interval == False:\n",
    "            # 统计标注\n",
    "            if real_type == 'Phenotype':\n",
    "                terms_not_covered_by_MetaMap.append( real_name )\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "291"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "len( terms_not_covered_by_MetaMap )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['irritability',\n 'muscles weak',\n 'muscles paralyzed',\n 'changes in mental status',\n 'paralytic disease']"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "terms_not_covered_by_MetaMap[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "97"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "# 观测未被覆盖的术语有多少已在训练集中标注过\n",
    "count = 0\n",
    "for term in terms_not_covered_by_MetaMap:\n",
    "    if term in terms_occurred_in_training_set:\n",
    "        count += 1\n",
    "count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表型属性模板的定义方法\n",
    "# 给定语料，提取包含有表型的句子，统计表型上下文中属性的出现情况\n",
    "# 根据属性的出现情况筛选属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[ConceptMMI(index='0', mm='MMI', score='3.75', preferred_name='Pneumonia', cui='C0032285', semtypes='[dsyn]', trigger='[\"Pneumonia\"-tx-1-\"lungs\"-noun-0]', location='TX', pos_info='102/5', tree_codes=''),\n ConceptMMI(index='0', mm='MMI', score='3.55', preferred_name='Abscess', cui='C0000833', semtypes='[dsyn]', trigger='[\"Abscess\"-tx-1-\"abscesses\"-noun-0]', location='TX', pos_info='78/9', tree_codes=''),\n ConceptMMI(index='0', mm='MMI', score='3.55', preferred_name='Pain', cui='C0030193', semtypes='[sosy]', trigger='[\"Pain\"-tx-1-\"painful\"-adj-0]', location='TX', pos_info='70/7', tree_codes=''),\n ConceptMMI(index='1', mm='MMI', score='3.68', preferred_name='Abscess', cui='C0000833', semtypes='[dsyn]', trigger='[\"Abscess\"-tx-1-\"abscesses\"-noun-0]', location='TX', pos_info='19/9', tree_codes=''),\n ConceptMMI(index='2', mm='MMI', score='3.50', preferred_name='Severe (severity modifier)', cui='C0205082', semtypes='[fndg]', trigger='[\"Severe\"-tx-1-\"severe\"-adj-0]', location='TX', pos_info='4/6', tree_codes='')]"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "# concepts = dict_of_concepts_in_texts[diseases_name]\n",
    "# concepts = mm.extract_concepts(text_sents, list(sents_idx)\n",
    "# concept.index --> text_sents[concept.index]\n",
    "# concept.pos_info 句子级别的位置 \n",
    "# finding attribute in the context around the concept\n",
    "diseases_name = \"Actinomycosis\"\n",
    "dict_of_concepts_in_texts[diseases_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 纳入全部的疾病\n",
    "sents_with_phenotypes = set()\n",
    "\n",
    "for disease_name in list_of_diseases:\n",
    "    # 该疾病文档的句子划分\n",
    "    file_of_original_text  = os.path.join( corpus_path, disease_name + '.txt' )\n",
    "    raw_text = open( file_of_original_text, 'r', encoding='utf-8').read() \n",
    "    raw_text = raw_text.replace('\\n',',')     \n",
    "\n",
    "    text_sents = []\n",
    "    text_sents_with_spans = []\n",
    "    for start, end in PunktSentenceTokenizer().span_tokenize(raw_text):\n",
    "        text_sents_with_spans.append( (start,end, raw_text[start:end]) )\n",
    "        text_sents.append( raw_text[start:end] )\n",
    "\n",
    "\n",
    "    # 获取该疾病文本的机器标注\n",
    "    dict_of_meta_annotations = read_metamap_annotation_result( raw_text, dict_of_concepts_in_texts[disease_name] )\n",
    "    # 只保留其中HPO表型异常的部分\n",
    "    dict_of_meta_annotations_cleaned = clean_metamap_annotations_with_hpo( dict_of_meta_annotations, \\\n",
    "                                            list_of_excluded_cuis_in_hpo, semantic_group_of_disorder, semantic_group_of_anatomy )    \n",
    "\n",
    "\n",
    "    # \n",
    "    for span_key_of_metamap in dict_of_meta_annotations_cleaned:\n",
    "        # metamap注释的概念\n",
    "        concept = dict_of_meta_annotations_cleaned[span_key_of_metamap]\n",
    "        # 定位该表型概念所在的句子索引\n",
    "        sent_idx = int( concept.index )\n",
    "        # 记录该句子\n",
    "        sents_with_phenotypes.add( text_sents[sent_idx] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1688"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "# 合计得到1688个句子\n",
    "len( sents_with_phenotypes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Signs and symptoms,Signs and symptoms of PCP include fever, nonproductive cough (because sputum is too viscous to become productive), shortness of breath (especially on exertion), weight loss, and night sweats.'"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "# list(sents_with_phenotypes)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入属性库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_lib_path = \"/home/denglizong/SSUMiner/corpus/AttributeLibrary\"\n",
    "snomed_attributes_file = os.path.join( attribute_lib_path, \"snomed_attributes_lib_filted.json\" )\n",
    "hpo_attributes_file    = os.path.join( attribute_lib_path, \"hpo_attributes_lib_filted.json\" )\n",
    "icd_attributes_file    = os.path.join( attribute_lib_path, \"icd_attributes_lib.json\" )\n",
    "cem_attributes_file    = os.path.join( attribute_lib_path, \"cem_attributes_lib.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入属性库\n",
    "dict_of_snomed_attributes = json.load( open(snomed_attributes_file,'r',encoding='utf-8') )\n",
    "# dict_of_snomed_attributes\n",
    "dict_of_hpo_attributes = json.load( open(hpo_attributes_file,'r',encoding='utf-8') )\n",
    "#\n",
    "dict_of_icd_attributes = json.load( open(icd_attributes_file,'r',encoding='utf-8') )\n",
    "#\n",
    "dict_of_cem_attributes = json.load( open(cem_attributes_file,'r',encoding='utf-8') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_of_cem_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "334"
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "# 合并属性库\n",
    "dict_of_attributes = {}\n",
    "dict_of_attributes.update( dict_of_snomed_attributes )\n",
    "dict_of_attributes.update( dict_of_hpo_attributes )\n",
    "dict_of_attributes.update( dict_of_icd_attributes )\n",
    "dict_of_attributes.update( dict_of_cem_attributes )\n",
    "len(dict_of_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Fatal',\n 'Life threatening severity',\n 'Mild',\n 'Mild to moderate',\n 'Moderate',\n 'Moderate to severe',\n 'Not severe',\n 'Severe']"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "dict_of_attributes[\"272141005::Severities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 属性库中有342个属性，应该挑选哪一些属性及属性槽来配置属性模板呢？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 肯定不纳入属性模板的属性 \n",
    "excluded_attributes = ['272068007::Negative integer',\"272070003::Ordinal number\",\"272150007::Openness\",\n",
    "                        \"272126005::Order values\",\"272127001::Event orders\",\"272146000::Uniformities\", \"57615005::Definite time\",\n",
    "                        \"310886004::Absolute times - hours\", \"314804007::Cardiovascular site descriptor\",\"272147009::Velocities\",\n",
    "                        \"303109001::Pathogeneses\",\"314808005::Oral site descriptor\",\"314809002::Urinary site descriptor\",\n",
    "                        \"314806009::Respiratory site descriptor\",\"314805008::Digestive site descriptor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "319"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "# 先从属性库中删除这些属性\n",
    "for key in excluded_attributes:\n",
    "    if key in dict_of_attributes:\n",
    "        del dict_of_attributes[key]\n",
    "\n",
    "len(dict_of_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"314806009::Respiratory site descriptor\" in dict_of_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "37\n"
    }
   ],
   "source": [
    "# 机器初筛：在至少两个不同的句子中出现，且出现的属性取值是不相同的。\n",
    "\n",
    "# 符合条件的候选属性个数\n",
    "count_of_candidate_attributes = 0\n",
    "\n",
    "# 符合条件的候选属性信息 list of tmpdict\n",
    "info_of_candidate_attributes = []\n",
    "\n",
    "\n",
    "for name_of_attribute in dict_of_attributes:\n",
    "    #\n",
    "    if name_of_attribute in excluded_attributes:\n",
    "        continue\n",
    "\n",
    "    # values \n",
    "    values_of_attribute = dict_of_attributes[name_of_attribute]\n",
    "    # 如果属性的取值列表过长(超过30个), 跳过\n",
    "    if len(values_of_attribute) >=25:\n",
    "        continue\n",
    "\n",
    "    # creat a keyword_processor for this attribute\n",
    "    keyword_processor = KeywordProcessor()\n",
    "\n",
    "    # keyword_processor.add_keyword(<unclean name/替换起点>, <standardised name/替换终点>)\n",
    "    for value in values_of_attribute:\n",
    "        # severe --> 【severe】\n",
    "        keyword_processor.add_keyword( value, '【' + value + '】' ) \n",
    "\n",
    "    # occurences of values in sentences\n",
    "    # 1. 统计该属性出现在了多少句子中 (general)\n",
    "    occurence_of_attribute_in_sents = 0\n",
    "    # 2. 统计该属性的取值的出现次数 (在一个句子中出现多次仅算一次，以便与属性的统计统一)\n",
    "    occurence_of_values_in_sents = {}\n",
    "    for value in values_of_attribute:\n",
    "        occurence_of_values_in_sents.setdefault( value.lower() , 0)\n",
    "    # 3. 记录n(10)个包含该属性某一取值的句子，记录该句子包含的该属性的1个取值\n",
    "    sentences_containing_attribute = []\n",
    "    # 注意sent不是一个string\n",
    "    for sent in sents_with_phenotypes:\n",
    "        # 搜索句子中出现的属性取值\n",
    "        # keyword_processor.add_keyword('is', 'IS')\n",
    "        # keyword_processor.extract_keywords('This is a problem')\n",
    "        # ['IS']\n",
    "        # keyword_processor.replace_keywords('This is a problem')\n",
    "        # 'This IS a problem'\n",
    "        hits = keyword_processor.extract_keywords(sent)\n",
    "        if len(hits) > 0:\n",
    "            # 如果搜索到了属性取值，那么该属性的出现次数+1\n",
    "            occurence_of_attribute_in_sents += 1\n",
    "            # 记录出现了该属性的句子，及其中出现的属性取值\n",
    "            if len(sentences_containing_attribute) <= 10:\n",
    "                # sentences_containing_attribute.setdefault( hits[0], sent )\n",
    "                # sent marked up with value\n",
    "                marked_sent = keyword_processor.replace_keywords(sent)\n",
    "                sentences_containing_attribute.append(marked_sent)\n",
    "            # 如果某一取值存在，统计数+1\n",
    "            # hits ['【Early stage】']  hit[1:-1]\n",
    "            lower_hits = [hit[1:-1].lower() for hit in hits]\n",
    "            for occurred_value in set(lower_hits):\n",
    "                if occurred_value in occurence_of_values_in_sents:\n",
    "                    occurence_of_values_in_sents[occurred_value] += 1\n",
    "    # 属性筛选\n",
    "    # 至少出现在两个句子中；至少有两个取值出现。\n",
    "    # 为了避免假阳性/偶然性，一个有效的取值至少应该出现在2个句子中\n",
    "    # 一个属性至少具有2个这样的取值\n",
    "    keep_flag = False\n",
    "    number_of_qualified_value = 0\n",
    "    for occurred_value in occurence_of_values_in_sents:\n",
    "        # 为了避免假阳性/偶然性，一个有效的取值至少应该出现在2个句子中\n",
    "        if occurence_of_values_in_sents[occurred_value] >= 2:\n",
    "            number_of_qualified_value += 1\n",
    "    # 如果一个属性至少具有2个这样的取值，那这一属性存在的可能性非常大了\n",
    "    if number_of_qualified_value >=2 :\n",
    "        keep_flag = True\n",
    "\n",
    "    # output observing results\n",
    "    if keep_flag:\n",
    "        # print(\"候选属性名称: \", name_of_attribute)\n",
    "        # print(\"该属性在句子中的出现次数：\", occurence_of_attribute_in_sents)\n",
    "        # print(\"该属性的取值在语料中的分布：\", occurence_of_values_in_sents)\n",
    "        # print(\"包含该属性的句子证据:\")\n",
    "        # for index, sent in enumerate(sentences_containing_attribute) :\n",
    "        #     print(index, sent)\n",
    "        # break\n",
    "        count_of_candidate_attributes += 1\n",
    "        # info of candidate attribute\n",
    "        tmpdict = {}\n",
    "        tmpdict.setdefault(\"name\", name_of_attribute )\n",
    "        tmpdict.setdefault(\"occurrences\", occurence_of_attribute_in_sents )\n",
    "        # 取值分布按出现次数排列 (精简下显示模式) [ (\"covered\",5), (\"unaided\",2)]\n",
    "        distribution_of_values_sorted = sorted(occurence_of_values_in_sents.items(), key=lambda item:item[1], reverse=True)\n",
    "        number_of_occurred_values = 0\n",
    "        dict_of_distribution_of_values = {}\n",
    "        for _value, _count in distribution_of_values_sorted:\n",
    "            dict_of_distribution_of_values.setdefault(_value, _count)\n",
    "            if _count >= 1:\n",
    "                number_of_occurred_values +=1\n",
    "        tmpdict.setdefault(\"distributions\", dict_of_distribution_of_values )\n",
    "        # 在语料中出现过的属性取值占所有取值数目的百分比\n",
    "        percentage_of_occurred_values = round(number_of_occurred_values/len(distribution_of_values_sorted), 2)\n",
    "        tmpdict.setdefault(\"percentage\", percentage_of_occurred_values )\n",
    "        # evidence of sentences\n",
    "        tmpdict.setdefault(\"evidences\", sentences_containing_attribute )\n",
    "        #\n",
    "        info_of_candidate_attributes.append( tmpdict )\n",
    "        \n",
    "print(count_of_candidate_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'name': '106240007::General clinical stage for disease AND/OR neoplasm',\n 'occurrences': 5,\n 'distributions': {'early stage': 3,\n  'late stage': 2,\n  'end-stage': 0,\n  'histologic grading differentiation and/or behavior': 0,\n  'midstage': 0,\n  'stage level 1': 0,\n  'stage level 2': 0,\n  'stage level 3': 0,\n  'stage level 4': 0,\n  'stage level 5': 0},\n 'percentage': 0.2,\n 'evidences': ['[20],,Early complications,Additional problems may occur in the 【Early stage】 of the illness.',\n  'Patients in the 【Late stage】 had significant thrombocytopenia, and deficiency of coagulation factors was less severe than in the early form.',\n  'Very low blood pressure may occur at an 【Early stage】, especially but not exclusively in meningococcal meningitis; this may lead to insufficient blood supply to other organs.',\n  '[20],,,,Early complications,Additional problems may occur in the 【Early stage】 of the illness.',\n  'Most people with the 【Late stage】 form died within 8 to 12 days of illness.']}"
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "info_of_candidate_attributes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(189, 'HP:0011008::Temporal pattern'),\n (164, '272141005::Severities'),\n (158, 'ICD::Course'),\n (152, 'ICD::Mild Moderate Severe Scale Value'),\n (152, 'HP:0012824::Severity')]"
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "source": [
    "# 根据属性的出现次数进行排序\n",
    "sorted_occurrence_of_attributes = []\n",
    "for infodict in info_of_candidate_attributes:\n",
    "    sorted_occurrence_of_attributes.append( (infodict[\"occurrences\"], infodict[\"name\"]) )\n",
    "\n",
    "sorted_occurrence_of_attributes.sort(reverse=True)\n",
    "sorted_occurrence_of_attributes[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据occurence排序后的结果文件\n",
    "sorted_info_of_candidate_attributes = []\n",
    "\n",
    "for (count, name_of_attribute) in sorted_occurrence_of_attributes:\n",
    "    # \n",
    "    for infodict in info_of_candidate_attributes:\n",
    "        if infodict[\"name\"] == name_of_attribute:\n",
    "            sorted_info_of_candidate_attributes.append( infodict )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/home/denglizong/SSUMiner'"
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 info_of_candidate_attributes 看看\n",
    "file_of_candidate_attributes = os.path.join( attribute_lib_path, 'results_of_candidate_attributes.json' )\n",
    "\n",
    "json.dump(sorted_info_of_candidate_attributes,\n",
    "          open(file_of_candidate_attributes,'w', encoding='utf-8'), ensure_ascii=False, indent=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}